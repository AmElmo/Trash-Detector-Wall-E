{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jG4maXUgAf9"
      },
      "source": [
        "# **Roadmap**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxH_VmP1gOMc"
      },
      "source": [
        "1. Install the TensorFlow Object Detection API\n",
        "2. Setup folder structure\n",
        "3. Generate the TFRecord files required for training\n",
        "4. Edit the model pipeline config file and download the pre-trained model checkpoint\n",
        "5. Train and evaluate the model\n",
        "\n",
        "⚠️ This notebook is meant to be run in Google Colab for training in order to use GPU capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nTZwdPIkgDhC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "# Check GPU setup\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABxZbnKngnQQ",
        "outputId": "0078f0d0-c90b-4787-b68b-42cd97b1b7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "# Check RAM setup\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIzxfIMHg34x"
      },
      "source": [
        "# **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNjiARb_g2J0",
        "outputId": "364514a9-5ecb-464f-cff5-5461df93f42f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "import json\n",
        "import shutil\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L37Fm-cMhBjP"
      },
      "source": [
        "# 1. Create customTF2, training and data folders in your google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RzMtcv3h7On"
      },
      "source": [
        "Create a folder named ***customTF2***.\n",
        "\n",
        "Create another folder named ***training*** inside the ***customTF2*** folder\n",
        "(***training*** folder is where the checkpoints will be saved during training)\n",
        "\n",
        "Create another folder named ***data*** inside the ***customTF2*** folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Mount drive and link your folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Clone the tensorflow models git repository & Install TensorFlow Object Detection API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# clone the tensorflow models on the colab cloud vm\n",
        "!git clone --q https://github.com/tensorflow/models.git\n",
        "\n",
        "#navigate to /models/research folder to compile protos\n",
        "%cd models/research\n",
        "\n",
        "# Compile protos.\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# Install TensorFlow Object Detection API.\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing the model builder\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Train / Test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the following repy and script (https://github.com/akarazniewicz/cocosplit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Command to split 80%\n",
        "\n",
        "⬛️ python cocosplit.py --having-annotations -s 0.8 '/Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset/final_annotations_coco_V6.json' '/Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset/train.json' '/Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset/test.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Convert Train & Test to TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLI command we use the fiftyone library for this\n",
        "\n",
        "fiftyone convert \\\n",
        "            --input-dir data/Final_Dataset/test.json \\\n",
        "            --input-type fiftyone.types.COCODetectionDataset \\\n",
        "            --output-dir data/Final_Dataset/TFRecord \\\n",
        "            --output-type fiftyone.types.TFObjectDetectionDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read the JSON to get the path\n",
        "# iterate over and copy the file to new folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_train_test_folders(json_file,path_to_original_data,path_to_new_split_data,name_of_split):\n",
        "    with open(json_file) as f:\n",
        "        json_annotations = json.load(f)\n",
        "    \n",
        "    for image in json_annotations['images']:\n",
        "        path_image = image['file_name']\n",
        "        os.makedirs(os.path.dirname(f\"{path_to_new_split_data}/{name_of_split}/{path_image}\"), exist_ok=True)\n",
        "        shutil.copy2(f\"{path_to_original_data}/{path_image}\", f\"{path_to_new_split_data}/{name_of_split}/{path_image}\")\n",
        "    \n",
        "    return f\"Completed transfer of {name_of_split}\"\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Completed transfer of test'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_train_test_folders('/Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset/test.json', '/Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset', '/Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset_Split', 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coco-split \\\n",
        "    --has_annotations \\\n",
        "    --test_ratio .2 \\\n",
        "    --valid_ratio 0 \\\n",
        "    --annotations_file '/Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset/final_annotations_coco_V6.json' \\\n",
        "    --train_name 'Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset_Split/train.json' \\\n",
        "    --test_name 'Users/julienberthomier/code/AmElmo/Main_Projects/Trash Detector (Wall-E)/data/Final_Dataset_Split/test.json'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('trash_detector_wall-e')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a9005b8f1337cfce9ec6fa0492a6417dcf5d256ddeac4739f2e09b90b3753a40"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
